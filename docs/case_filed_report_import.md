# CaseFiledRPT Manual Import (Render Postgres)

This repo supports importing PACER "Case Filed Report" exports (pipe-delimited `.txt`) into the production Postgres database.

This workflow is intentionally **client-side**:
- The report file stays on your machine.
- `render psql` connects to Render Postgres and uses `\copy` to stream CSV data from your machine into temporary tables.

## What This Import Does

For each input report:

1. **Parse + normalize** the report.
2. **Aggregate** rows into case-level records + party rows.
3. **Upsert** cases into `pcl_cases` keyed by `(court_id, case_number_full)`.
4. **Insert** parties into `pcl_parties` using `record_hash` de-duplication.

Primary goal: ensure every case has a usable `case_id` (ECF case id) + `case_link`, so docket pulls can be queued reliably.

## Output Artifacts

Generated under `output/spreadsheet/case_filed_report/<DATE>_<COURT>_<FILENAME_STEM>/`:

- `<court>_rows_normalized.csv`
  - Row-level normalized copy of the report.
- `<court>_cases_review.csv`
  - Human-friendly, case-level listing (what you typically review).
- `<court>_cases_import.csv`
  - Import-ready case rows for `pcl_cases`.
- `<court>_parties_basic.csv`
  - Party rows keyed by `case_number_full` (resolved to `case_id` later).
- `<court>_cases_upsert.sql`
  - psql script that upserts cases + writes `<court>_case_id_mapping.tsv`.
- `<court>_case_id_mapping.tsv`
  - Generated by running the upsert SQL; maps `case_number_full -> pcl_cases.id`.
- `*/parties_import.csv`, `*/parties_insert.sql`
  - Generated from the mapping file for `pcl_parties` inserts.

Everything in `output/` is gitignored.

## Step-by-Step

### 1) Prepare import files

```bash
python3 scripts/prepare_case_filed_report_import.py \
  --court-id paedc \
  --input "/Users/david/Downloads/CaseFiledRPT (2).txt"
```

Repeat for other courts, e.g.:

```bash
python3 scripts/prepare_case_filed_report_import.py \
  --court-id vidc \
  --input "/Users/david/Downloads/CaseFiledRPT (3).txt"
```

### 2) Upsert cases into production DB

Run the generated SQL via Render:

```bash
render psql dpg-d5o27kt6ubrc73f7nlfg-a -- -v ON_ERROR_STOP=1 -f \
  output/spreadsheet/case_filed_report/<RUN_DIR>/<court>_cases_upsert.sql
```

This will also write `<court>_case_id_mapping.tsv` into the same run directory.

### 3) Prepare party inserts using the mapping

```bash
python3 scripts/prepare_case_filed_report_parties_import.py \
  --mapping output/spreadsheet/case_filed_report/<RUN_DIR>/<court>_case_id_mapping.tsv \
  --parties-basic output/spreadsheet/case_filed_report/<RUN_DIR>/<court>_parties_basic.csv \
  --out-dir output/spreadsheet/case_filed_report/<RUN_DIR>/<court>_parties
```

### 4) Insert parties into production DB

```bash
render psql dpg-d5o27kt6ubrc73f7nlfg-a -- -v ON_ERROR_STOP=1 -f \
  output/spreadsheet/case_filed_report/<RUN_DIR>/<court>_parties/parties_insert.sql
```

## Sanity Checks (Useful Queries)

```sql
-- Total cases
select count(*) from pcl_cases;

-- Missing ECF case_id (docket pulls need this)
select count(*)
from pcl_cases
where court_id='paedc' and case_type='cr'
  and (case_id is null or case_id='');

-- Judge filtering (example)
select count(*)
from pcl_cases
where court_id='paedc' and case_type='cr'
  and lower(judge_last_name)='kearney';
```

